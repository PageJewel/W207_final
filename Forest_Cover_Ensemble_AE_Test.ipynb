{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "name": "",
  "signature": "sha256:bb0a4913f3f2a98cdd02819b3efe0a1afee2a69257e8ef0352eb753b79c73306"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Forest Cover Type: Ensemble on Test Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this project, we are working on predicting the forest cover type among 7 classifications based on cartographic variables from the US Geological Survey and USFS for each 30 x 30 meter cell of forest.\n",
      "\n",
      "For more details, please see: https://www.kaggle.com/c/forest-cover-type-prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Import libraries**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This tells matplotlib not to try opening a new window for each plot.\n",
      "%matplotlib inline\n",
      "\n",
      "# General libraries.\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.style\n",
      "matplotlib.style.use('ggplot')\n",
      "\n",
      "# SK-learn libraries for learning.\n",
      "from sklearn import preprocessing\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.naive_bayes import BernoulliNB\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier \n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "\n",
      "# SK-Learn Libraries for feature tuning\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "\n",
      "# SK-learn libraries for evaluation.\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import classification_report"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Load Data**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load full training data set\n",
      "full_data = np.loadtxt(\"train.csv\", dtype = \"int\", delimiter = \",\", skiprows=1)\n",
      "feature_names = np.loadtxt(\"train.csv\", dtype = \"str\", delimiter = \",\")[0,:]\n",
      "feature_names = np.delete(feature_names, [0,55]) # Remove ID and output variable to match data set\n",
      "\n",
      "# Split into data and labels\n",
      "full_data_labels = full_data[:,full_data.shape[1]-1]\n",
      "full_data = full_data[:,:full_data.shape[1]-1]\n",
      "full_data = np.delete(full_data, 0, 1)  # Delete id to prevent use as feature\n",
      "\n",
      "train_data = full_data\n",
      "train_labels = full_data_labels\n",
      "\n",
      "print (\"full data shape: \", full_data.shape)\n",
      "print (\"full label shape:\", full_data_labels.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "full data shape:  (15120, 54)\n",
        "full label shape: (15120,)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Load test data**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load test data set\n",
      "full_test_data = np.loadtxt(\"test.csv\", dtype = \"int\", delimiter = \",\", skiprows=1)\n",
      "\n",
      "test_data = np.delete(full_test_data, 0, 1)  # Delete id to prevent use as feature\n",
      "\n",
      "print (\"test data shape: \", test_data.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test data shape:  (565892, 54)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Preprocessing**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Confirm features with 0 variation\n",
      "print(\"For feature %s there are %d unique values in the training data\" % (feature_names[20], np.unique(train_data[:,20]).shape[0]))\n",
      "print(\"For feature %s there are %d unique values in the training data\" % (feature_names[21], np.unique(train_data[:,21]).shape[0]))\n",
      "print(\"For feature %s there are %d unique values in the training data\" % (feature_names[28], np.unique(train_data[:,28]).shape[0]))\n",
      "\n",
      "# Remove those features - Note: only run this section once\n",
      "train_data = np.delete(train_data, [20,21,28], 1)\n",
      "print (\"\\ntrain data shape: \", train_data.shape)\n",
      "\n",
      "test_data = np.delete(test_data, [20,21,28], 1)\n",
      "print (\"\\ndev data shape: \", test_data.shape)\n",
      "\n",
      "feature_names = np.delete(feature_names, [20,21,28])\n",
      "print(\"\\nFeature names are:\")\n",
      "print(feature_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "For feature Soil_Type7 there are 1 unique values in the training data\n",
        "For feature Soil_Type8 there are 2 unique values in the training data\n",
        "For feature Soil_Type15 there are 1 unique values in the training data\n",
        "\n",
        "train data shape:  (15120, 51)\n",
        "\n",
        "dev data shape: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (565892, 51)\n",
        "\n",
        "Feature names are:\n",
        "['Elevation' 'Aspect' 'Slope' 'Horizontal_Distance_To_Hydrology'\n",
        " 'Vertical_Distance_To_Hydrology' 'Horizontal_Distance_To_Roadways'\n",
        " 'Hillshade_9am' 'Hillshade_Noon' 'Hillshade_3pm'\n",
        " 'Horizontal_Distance_To_Fire_Points' 'Wilderness_Area1'\n",
        " 'Wilderness_Area2' 'Wilderness_Area3' 'Wilderness_Area4' 'Soil_Type1'\n",
        " 'Soil_Type2' 'Soil_Type3' 'Soil_Type4' 'Soil_Type5' 'Soil_Type6'\n",
        " 'Soil_Type9' 'Soil_Type10' 'Soil_Type11' 'Soil_Type12' 'Soil_Type13'\n",
        " 'Soil_Type14' 'Soil_Type16' 'Soil_Type17' 'Soil_Type18' 'Soil_Type19'\n",
        " 'Soil_Type20' 'Soil_Type21' 'Soil_Type22' 'Soil_Type23' 'Soil_Type24'\n",
        " 'Soil_Type25' 'Soil_Type26' 'Soil_Type27' 'Soil_Type28' 'Soil_Type29'\n",
        " 'Soil_Type30' 'Soil_Type31' 'Soil_Type32' 'Soil_Type33' 'Soil_Type34'\n",
        " 'Soil_Type35' 'Soil_Type36' 'Soil_Type37' 'Soil_Type38' 'Soil_Type39'\n",
        " 'Soil_Type40']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data_subset1 = test_data[:100000,:]\n",
      "print(test_data_subset1.shape)\n",
      "\n",
      "test_data_subset2 = test_data[100000:200000,:]\n",
      "print(test_data_subset2.shape)\n",
      "\n",
      "test_data_subset3 = test_data[200000:300000,:]\n",
      "print(test_data_subset3.shape)\n",
      "\n",
      "test_data_subset4 = test_data[300000:400000,:]\n",
      "print(test_data_subset4.shape)\n",
      "\n",
      "test_data_subset5 = test_data[400000:500000,:]\n",
      "print(test_data_subset5.shape)\n",
      "\n",
      "test_data_subset6 = test_data[500000:565892,:]\n",
      "print(test_data_subset6.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(100000, 51)\n",
        "(100000, 51)\n",
        "(100000, 51)\n",
        "(100000, 51)\n",
        "(100000, 51)\n",
        "(65892, 51)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Scaling\n",
      "def my_scaler(data):    \n",
      "    scaler = preprocessing.StandardScaler()\n",
      "    continuous = scaler.fit_transform(data[:,:10])\n",
      "    binary = data[:,10:]\n",
      "    return np.concatenate((continuous, binary),axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Feature selection\n",
      "def my_featureselection(num_features, fit_data, fit_labels, transform_data):\n",
      "    selection = SelectKBest(k=num_features)\n",
      "    top_train = selection.fit_transform(fit_data,fit_labels)\n",
      "    top_dev = selection.transform(transform_data)\n",
      "    return [top_train, top_dev]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set up scaling of all original data and selection of top 25 variables only\n",
      "scaled_train_data = my_scaler(train_data)\n",
      "scaled_test_data = my_scaler(test_data)\n",
      "top25_train_data,top25_test_data = my_featureselection(25,train_data,train_labels,test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python3/dist-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "/usr/lib/python3/dist-packages/sklearn/utils/__init__.py:127: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
        "  if np.issubdtype(mask.dtype, np.int):\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Best Naive Bayes**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate standard deviation of all binary features in training data (regardless of which are in the model)\n",
      "# This is across all observations / classes\n",
      "sd_data = np.std(train_data[:,10:])\n",
      "\n",
      "# Create model\n",
      "model_NB = GaussianNB()\n",
      "model_NB.fit(top25_train_data,train_labels)\n",
      "   \n",
      "# Set up new sigma array\n",
      "new_sigma = model_NB.sigma_\n",
      "        \n",
      "# For each binary feature, reset its sigma value to the quantity provided\n",
      "for i in range(top25_train_data.shape[1]):\n",
      "    if np.unique(top25_train_data[:,i]).shape[0]==2:\n",
      "        new_sigma[:,i] = sd_data\n",
      "        \n",
      "# Replace sigma array for the model\n",
      "model_NB.sigma_ = new_sigma\n",
      "\n",
      "# get predicted probs\n",
      "predict_NB_train = model_NB.predict_proba(top25_train_data)\n",
      "predict_NB_test = model_NB.predict_proba(top25_test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Logistic Regression**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the model\n",
      "model_MN = LogisticRegression(C=3, class_weight='balanced',multi_class='multinomial',solver='lbfgs',max_iter=250)\n",
      "model_MN.fit(scaled_train_data, train_labels)\n",
      "\n",
      "# predict probs\n",
      "predict_MN_train = model_MN.predict_proba(scaled_train_data)\n",
      "predict_MN_test = model_MN.predict_proba(scaled_test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Best KNN**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the model\n",
      "model_knn = KNeighborsClassifier(n_neighbors=3,metric='braycurtis')\n",
      "model_knn.fit(train_data, train_labels)\n",
      "\n",
      "# predict probs\n",
      "predict_knn_train = model_knn.predict_proba(train_data)\n",
      "predict_knn_test = model_knn.predict_proba(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Random Forest**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create the model\n",
      "model_rf = RandomForestClassifier(n_estimators=250,max_features=25)\n",
      "model_rf.fit(train_data, train_labels)\n",
      "\n",
      "# predict probs - computer crashed on full test set, so broke into smaller chunks below\n",
      "predict_rf_train = model_rf.predict_proba(train_data)\n",
      "#predict_rf_test = model_rf.predict_proba(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict_rf_test1 = model_rf.predict_proba(test_data_subset1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict_rf_test2 = model_rf.predict_proba(test_data_subset2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict_rf_test3 = model_rf.predict_proba(test_data_subset3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict_rf_test4 = model_rf.predict_proba(test_data_subset4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict_rf_test5 = model_rf.predict_proba(test_data_subset5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict_rf_test6 = model_rf.predict_proba(test_data_subset6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predict_rf_test = np.vstack((predict_rf_test1, predict_rf_test2, predict_rf_test3, predict_rf_test4, predict_rf_test5, predict_rf_test6))\n",
      "\n",
      "print(predict_rf_test.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(565892, 7)\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Ensemble - include predicted probabilities into knn**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run best KNN on mix of predicted probs and scaled original data\n",
      "new_train_data = np.hstack((scaled_train_data, predict_NB_train, predict_MN_train, predict_rf_train, predict_knn_train))\n",
      "new_test_data = np.hstack((scaled_test_data, predict_NB_test, predict_MN_test, predict_rf_test, predict_knn_test))\n",
      "\n",
      "final_model = KNeighborsClassifier(n_neighbors=1,metric='braycurtis')\n",
      "final_model.fit(new_train_data, train_labels)\n",
      "\n",
      "# predict probabilities - crashed my computer, so split these up\n",
      "#test_preds = final_model.predict(new_test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='braycurtis',\n",
        "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
        "           weights='uniform')"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data_subset1 = new_test_data[:100000,:]\n",
      "test_data_subset2 = new_test_data[100000:200000,:]\n",
      "test_data_subset3 = new_test_data[200000:300000,:]\n",
      "test_data_subset4 = new_test_data[300000:400000,:]\n",
      "test_data_subset5 = new_test_data[400000:500000,:]\n",
      "test_data_subset6 = new_test_data[500000:565892,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_preds1 = final_model.predict(test_data_subset1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_preds2 = final_model.predict(test_data_subset2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_preds3 = final_model.predict(test_data_subset3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_preds4 = final_model.predict(test_data_subset4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_preds5 = final_model.predict(test_data_subset5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_preds6 = final_model.predict(test_data_subset6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_preds = np.concatenate((test_preds1, test_preds2, test_preds3, test_preds4, test_preds5, test_preds6))\n",
      "\n",
      "print(test_preds.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(565892,)\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# output predictions for kaggle\n",
      "ids = np.asarray(full_test_data[:,0])\n",
      "cover_type = np.asarray(test_preds)\n",
      "dat_submit = pd.DataFrame(np.column_stack((ids,cover_type)))\n",
      "dat_submit.to_csv(\"test_predictions.csv\",header=[\"Id\",\"Cover_Type\"],index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    }
   ],
   "metadata": {}
  }
 ]
}